import sys
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, lit, lower, trim, regexp_replace, split, 
    when, concat_ws, substring, levenshtein, expr,
    broadcast, coalesce, first, struct, array, explode
)
from pyspark.sql.types import DoubleType
from pyspark.sql.window import Window


def normalize_name(name_col):
    """Normalise un nom : minuscules, sans accents, sans points"""
    normalized = lower(trim(name_col))
    # Supprimer les points et tirets
    normalized = regexp_replace(normalized, r"[.\-']", " ")
    # Supprimer les accents
    normalized = regexp_replace(normalized, "[àáâãäå]", "a")
    normalized = regexp_replace(normalized, "[èéêë]", "e")
    normalized = regexp_replace(normalized, "[ìíîï]", "i")
    normalized = regexp_replace(normalized, "[òóôõö]", "o")
    normalized = regexp_replace(normalized, "[ùúûü]", "u")
    normalized = regexp_replace(normalized, "[ýÿ]", "y")
    normalized = regexp_replace(normalized, "[ñ]", "n")
    normalized = regexp_replace(normalized, "[ç]", "c")
    # Supprimer les espaces multiples
    normalized = regexp_replace(normalized, r"\s+", " ")
    return trim(normalized)


def extract_last_name(name_col):
    """Extrait le nom de famille (dernier mot)"""
    words = split(name_col, " ")
    return words.getItem(expr("size(words) - 1"))


def extract_first_initial(name_col):
    """Extrait la première lettre du prénom"""
    return substring(name_col, 1, 1)


def combine_and_calculate_value_index(datalake_path: str, date_str: str):
    """
    Combine les données API-Football et Transfermarkt avec matching intelligent
    """
    spark = SparkSession.builder \
        .appName("FootballAnalytics_Combine") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()

    stats_path = os.path.join(datalake_path, f"formatted/{date_str}/stats_formatted")
    values_path = os.path.join(datalake_path, f"formatted/{date_str}/values_formatted")
    output_path = os.path.join(datalake_path, f"usage/{date_str}/football_analytics_final.parquet")

    print(f"--- Démarrage Combinaison Spark pour {date_str} ---")
    print(f"STATS IN: {stats_path}")
    print(f"VALUES IN: {values_path}")

    try:
        # ==============================
        # 1) Charger les données
        # ==============================
        df_stats = spark.read.parquet(stats_path)
        df_values = spark.read.parquet(values_path)

        print(f"Stats chargées : {df_stats.count()} joueurs")
        print(f"Values chargées : {df_values.count()} joueurs")

        # ==============================
        # 2) Normaliser les noms
        # ==============================
        df_stats_norm = df_stats.withColumn(
            "name_normalized", normalize_name(col("player_name_api"))
        ).withColumn(
            "last_name", extract_last_name(col("name_normalized"))
        ).withColumn(
            "first_initial", extract_first_initial(col("name_normalized"))
        )

        df_values_norm = df_values.withColumn(
            "name_normalized", normalize_name(col("player_name"))
        ).withColumn(
            "last_name", extract_last_name(col("name_normalized"))
        ).withColumn(
            "first_initial", extract_first_initial(col("name_normalized"))
        )

        # ==============================
        # 3) Stratégie de matching multi-niveaux
        # ==============================
        
        # Niveau 1 : Matching exact sur nom normalisé
        df_match_exact = df_stats_norm.join(
            broadcast(df_values_norm),
            df_stats_norm.name_normalized == df_values_norm.name_normalized,
            "left"
        ).withColumn("match_type", lit("exact"))

        # Niveau 2 : Matching sur nom de famille + première lettre
        df_unmatched = df_match_exact.filter(col("market_value_eur").isNull())
        df_matched_exact = df_match_exact.filter(col("market_value_eur").isNotNull())

        df_match_lastname = df_unmatched.drop("market_value_eur", "market_value_raw", "scraped_url", "match_type") \
            .join(
                broadcast(df_values_norm.select(
                    col("last_name").alias("v_last_name"),
                    col("first_initial").alias("v_first_initial"),
                    "market_value_eur",
                    "market_value_raw",
                    "scraped_url"
                )),
                (df_unmatched.last_name == col("v_last_name")) & 
                (df_unmatched.first_initial == col("v_first_initial")),
                "left"
            ).drop("v_last_name", "v_first_initial") \
            .withColumn("match_type", lit("lastname_initial"))

        # Niveau 3 : Matching fuzzy (distance de Levenshtein)
        df_unmatched_2 = df_match_lastname.filter(col("market_value_eur").isNull())
        df_matched_lastname = df_match_lastname.filter(col("market_value_eur").isNotNull())

        # Pour le fuzzy matching, on crée un cross join limité par nom de famille
        df_fuzzy_candidates = df_unmatched_2.drop("market_value_eur", "market_value_raw", "scraped_url", "match_type") \
            .join(
                broadcast(df_values_norm.select(
                    col("last_name").alias("v_last_name"),
                    col("name_normalized").alias("v_name_normalized"),
                    "market_value_eur",
                    "market_value_raw",
                    "scraped_url"
                )),
                df_unmatched_2.last_name == col("v_last_name"),
                "left"
            )

        # Calculer la distance de Levenshtein
        df_fuzzy_scored = df_fuzzy_candidates.withColumn(
            "levenshtein_distance",
            levenshtein(col("name_normalized"), col("v_name_normalized"))
        )

        # Garder le meilleur match (distance minimale) par joueur
        window_spec = Window.partitionBy("player_id").orderBy(col("levenshtein_distance").asc())
        
        df_fuzzy_best = df_fuzzy_scored.withColumn(
            "rank", expr("row_number() over (partition by player_id order by levenshtein_distance)")
        ).filter(col("rank") == 1) \
         .filter(col("levenshtein_distance") <= 3) \
         .drop("v_last_name", "v_name_normalized", "levenshtein_distance", "rank") \
         .withColumn("match_type", lit("fuzzy"))

        # Niveau 4 : Pas de match trouvé
        df_unmatched_final = df_fuzzy_best.filter(col("market_value_eur").isNull()) \
            .withColumn("market_value_eur", lit(0.0)) \
            .withColumn("match_type", lit("no_match"))

        df_matched_fuzzy = df_fuzzy_best.filter(col("market_value_eur").isNotNull())

        # ==============================
        # 4) Combiner tous les résultats
        # ==============================
        df_combined = df_matched_exact.unionByName(df_matched_lastname, allowMissingColumns=True) \
                                      .unionByName(df_matched_fuzzy, allowMissingColumns=True) \
                                      .unionByName(df_unmatched_final, allowMissingColumns=True)

        # ==============================
        # 5) Calculer le Value Index
        # ==============================
        # Normaliser la valeur marchande (0-100)
        max_value = df_combined.agg({"market_value_eur": "max"}).collect()[0][0]
        
        df_final = df_combined.withColumn(
            "value_index",
            when(col("market_value_eur") > 0, 
                 (col("market_value_eur") / lit(max_value)) * 100
            ).otherwise(lit(0.0))
        ).withColumn(
            "event_time", lit(date_str)
        )

        # Sélectionner les colonnes finales
        df_output = df_final.select(
            "player_id",
            "player_name_api",
            "team_name",
            "league_name",
            "market_value_eur",
            "value_index",
            "match_type",
            "extraction_date",
            "event_time"
        )

        # ==============================
        # 6) Statistiques de matching
        # ==============================
        match_stats = df_output.groupBy("match_type").count().collect()
        
        print("\n=== Statistiques de matching ===")
        for row in match_stats:
            print(f"  {row['match_type']}: {row['count']} joueurs")

        total_matched = df_output.filter(col("market_value_eur") > 0).count()
        total_players = df_output.count()
        match_rate = (total_matched / total_players) * 100 if total_players > 0 else 0
        
        print(f"\nTaux de matching global: {match_rate:.1f}% ({total_matched}/{total_players})")

        # ==============================
        # 7) Sauvegarder
        # ==============================
        df_output.write.mode("overwrite").parquet(output_path)
        print(f"\n✅ Données combinées sauvegardées : {output_path}")
        print(f"   Total joueurs : {df_output.count()}")

        # Afficher quelques exemples
        print("\n=== Exemples de matching ===")
        df_output.select("player_name_api", "market_value_eur", "value_index", "match_type") \
                 .orderBy(col("value_index").desc()) \
                 .show(10, truncate=False)

        spark.stop()

    except Exception as e:
        print(f"❌ Erreur Combinaison Spark: {e}")
        import traceback
        traceback.print_exc()
        spark.stop()
        sys.exit(1)


if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: combine_data.py <datalake_path> <YYYYMMDD>")
        sys.exit(1)

    combine_and_calculate_value_index(sys.argv[1], sys.argv[2])
